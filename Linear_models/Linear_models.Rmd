---
title: "Linear models"
output: 
  html_document:
    keep_md: true
---

```{r setup, include = FALSE}

  knitr::opts_chunk$set(message = FALSE, warning = FALSE)

```


# Overview

For this tutorial we will be analyzing data from an actual clinical trial conducted here in Cork. You can read more about the study [here](https://doi.org/10.1016/j.ahj.2018.03.018), but in a nutshell the study enrolled patients who had suffered a very serious heart attack (known as a [STEMI](https://en.wikipedia.org/wiki/Myocardial_infarction)) and randomized them into one of 3 groups. Two of the groups received an injection of [IGF1](https://en.wikipedia.org/wiki/Insulin-like_growth_factor_1) right into their heart (at two different doses) while the third group received an inactive placebo injection. The goal of the study of course was to understand the causal effects on this IGF injection, especially with respect to [global left ventricular ejection faction](https://en.wikipedia.org/wiki/Ejection_fraction) (GLVEF), which is one of many indicators of heart function. 

For the analysis that follows, we will be using and comparing t-tests, ANOVA, and linear models, as well as doing some data visualization to make sure that the models and tests make sense with respect to what we "see" in the data. Along the way, we will also point out various aspects of coding with R and RStudio. 

# R Markdown files

The first thing to point out is that this tutorial was made using a special kind of script, called a Rmd file. You can access this file [here](https://raw.githubusercontent.com/CRFCSDAU/EH6126_data_analysis_tutorials/master/Linear_models/Linear_models.Rmd), and the easiest thing for you to do at this stage is just to copy and paste the code from that link into a new Rmd file (File -> New File -> R Markdown).

RMarkdown files are an example of ["literate programming"](https://en.wikipedia.org/wiki/Literate_programming), where we combine "human readable" text (like this overview), "computer readable" code, and all of the resulting outputs (e.g. tables and figures) into a single document. Further, this document can be "knit" into different formats for sharing, including html files, Word docx, and PDFs. This makes it very easy to share high quality statistical reports without having to cut and paste a bunch of stuff from different documents into the a single report. To learn more about RMarkdown, [check our the resouces here](https://rmarkdown.rstudio.com/). 


For now, the most important thing to know is that code in a Rmd file is contained in a chunk, like this:

```{r, results = "hide"}

# In a code chunk, comments that you don't want R to try and run should be
# proceeded by a hashmark #.

# Install/load packages

# Create a character vector object with the names of the packages you will need. 
  packs <- c("tidyverse", "knitr", "viridis", "broom", "pwr", "sjPlot") 

# Install those packages but only if they aren't already installed
  install.packages(packs[!packs %in% installed.packages()])
  
# Load all the pakages (i.e. check them out from your library of downloaded
# packages)
  lapply(packs, library, character.only = TRUE)
  
# Don't worry about what all this means - you'll learn!

```


# Download the dataset

The first thing we need to do in order to analyze some data is to get some data! The data for this tutorial are contained in a comma-separated-values (csv) file on the github page for this project. We can download the data using the code contained in the chunk below. 

```{r}

# Read in the data from the github page

  data <- read_csv(file = "https://raw.githubusercontent.com/CRFCSDAU/EH6126_data_analysis_tutorials/master/Linear_models/example.csv")

```

To help understand that code, you need to recognize that `read_csv` is a function from the `readr` **package**, which is one of the `tidyverse` packages that you installed and loaded in the first chunk above.

If you want, you can learn more about this function by typing `?read_csv` into your console. The most important things to know are that it accepts a web address for the `file` argument, and returns a **dataframe** (or tibble). In this case we have assigned that dataframe to an object called `data`, with the **assignment** operator `<-`.

Remember that almost all of R scripting (or coding) is just taking [objects](https://vimeo.com/220493412) (a character vector containing a web address in this case), putting them into a [function](https://vimeo.com/220490105), which in turn creates new objects (a dataframe in this example) which we can assign a name in our work space (in this case, the object called `data`). 


# Explore the data

Now that we have the data, we need to check it out a bit and make sure it all looks ok.

First, properly constructed datasets should include a row for each observation (e.g. a person, or a mouse, or a country, etc); and each column should contain exactly one characteristic about the observations (e.g. their height, weight, or GDP, etc). Importantly, each of these characteristics should only include a single type of information (a number, or a date, or a character value. etc.) - that means no mixing and matching within a variable. For example, you shouldn't ever have a variable for blood pressure where you might include "High, 162" as an entry. This should instead be two different variables, one reflecting the actual blood pressure value (e.g. 162) and a separate column for the categorized version (e.g. high vs low).

I have a very small video about this [here](https://www.youtube.com/watch?v=Ry2xjTBtNFE) which basically just parrots the main points from [this really important paper](https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1375989). 

So the first thing we should check is how many rows and columns the data has. Let's do that using so called "in line" code, where I will call 2 functions, `nrow` and `ncol` to get this information. Importantly, this code will be rendered out here in the human readable part of the file, not in a code chunk, so you'll only see it if you are looking at the actual Rmd file. 

The number of columns in this dataset is `r ncol(data)`, and the number of rows is `r nrow(data)`.

Sometimes the easiest thing to do with smaller dataset is just "look" at it with the `View` function. 

```{r}

# View(data) 
# I have "commented out" the code just above to view the data to stop it from
# running when I knit the Rmd file. So either highlight the code and click run,
# or remove the # from the start of the line and run the chunk.

# Running this code will open a viewer where you can see the data. You could
# achieve the same thing by clicking on the object in the environment widow.

```

Because we'll be working with different variables (columns) in the dataset, it's useful to know what their names are. 

```{r}

  names(data)

# Naming variables is generally recognized as a hard job! You want something
# relatively short but that's still informative.

```

The most important variables in this dataset are `arm`, which tells us which treatment each patient was allocated to, and `eos_glvef`, which is the primary outcome of interest measured at the end of the study. So let's have a look at them together. 

```{r}

# Make a nice plot the distribution of glvef by study arm. 
# We use a dotplot to show the actual values of the outcome, glvef. 
# We use a boxplot to help summarize the distribution of those values. 
# We mark the mean value in each group with a red point. 
 
  ggplot(data, aes(x = arm, y = eos_glvef)) +
    geom_dotplot(binaxis = "y", stackdir = "centerwhole") +
    geom_boxplot(width = 0.3, alpha = 0.5) +
    stat_summary(fun.y = mean, geom = "point", shape = 20, size = 10,
                 color = "red", fill = "red") 

```

We can see that patients in the active arm C, who received high dose IGF1, have a higher mean GLVEF at the end of the study, compared to patients who got the placebo (with the patients in arm B who got the low dose of IGF1 somewhere in the middle). However, it's worth pointing out that people in the two active arms didn't always have higher values - in fact the people with the some of the lowest values were in the active arms. 

For the next part, we are going to focus on the HD and placebo arms (we'll come back to the LD arm shortly). 

```{r}

  data_2 <- filter(data, arm != "Arm B (LD)")
  # This creates a new dataframe object called data_2 that has removed the rows
  # of data for the patient in arm B. 

```


Next, let's see what the mean and standard deviation of GLVEF at the end of the study were for each arm. 

```{r}

  group_by(data_2, arm) %>%
  summarise(
    mean = round(mean(eos_glvef, na.rm = TRUE), 2), 
    sd   = round(sd  (eos_glvef, na.rm = TRUE), 2)
  ) 

```

So we can see that the difference in means is about 4 percentage points (note that the unit of measurement for GLVEF is "percentage points" - that's very different from saying that the difference in mean values was 5 percent, which would reflect a relative difference, i.e. a **5%** reduction in 50 would be 47.5, which is a difference of **2.5 percentage points**). We can also see that the SD was larger as well, reflecting the increased spread of the data around the mean, which was evident in the plots above. 

# The t-test

Now we want to use a statistical test to help use understand how to interpret this difference in mean outcome. Remember that the entire study was set up in such a way that we can interpret this between-arm difference in means as the causal effect of the treatment with respect to the outcome. **But**...this is still just an **estimate** of that causal effect in a sample of observations, and because of sampling error, we should expect some error (or noise) in that estimate. A frequentist statistical test, like the t-test, is just asking, "Assuming there was no effect of this treatment, how often would I expect to see an effect like the one I actually saw just because of sampling error?" Despite what others may argue, this is a very reasonable question to ask, and one very important way to help us stop from fooling ourselves. 

Here is what a t-test looks like in R:

```{r}

  our_t_test <- t.test(eos_glvef ~ arm, data = data_2)

  our_t_test

```

What does this tell us? First, it gives a t statistic (`r round(our_t_test$statistic, 2)`) and the degrees of freedom (df) for the test (`r round(our_t_test$parameter, 2)`). You can revisit this [other tutorial for more explanation](https://github.com/CRFCSDAU/EH6126_data_analysis_tutorials/blob/master/Unit_1_Review/Frequentist_inference.md), but the key thing is that the df determines the nature of the sampling distribution under the null, against which we compare our observed t statistic in order to get a p-value (`r round(our_t_test$p.value, 2)`).  

If you look at the Rmd file for this tutorial, you will see that I used in-line code to give these numbers in the text above. That way, if something about the data changes, I can just re-run the code and the numbers will all be appropriately updated. Trust me...things almost always change at some point, at least at bit, so this is very handy. In fact, now I want to "re-code" by variable for study arm for that the results we see are for the active arm relative to the placebo arm. That way my t statistic will be positive, reflecting that mean GLVEF was higher in the active arm, which is better. 

```{r}

  our_t_test <- data_2 %>%                                       
  mutate(arm = factor(arm, levels = levels(factor(arm))[c(2, 1)])) %>% 
  t.test(eos_glvef ~ arm, data = .)                            

  our_t_test
  
  # Note the use of the function factor() above. R has a special object types 
  # for categorical variables called "factor". It's basically an underlying 
  # number, which can be used in calculations, with an attached character label, 
  # that can be understood by the humans. Each value in a factor is called a 
  # level, and the order of the levels can dictate how the variable is used in 
  # other functions. You can learn more here: https://r4ds.had.co.nz/factors.html

```

Now it gives us a positive t statistic (`r round(our_t_test$statistic, 2)`) and 95% confidence interval for the difference in means (`r round(our_t_test$conf.int[1], 2)` to `r round(our_t_test$conf.int[2], 2)`) which is centered on a mean difference of `r round(our_t_test$estimate[1] - our_t_test$estimate[2], 2)` percentage points, comparing the HD arm to the placebo arm. 

# Z-test

Just for comparison, let's calculate a Z-test. It's the exact same idea as the t-test, except the sampling disruption under the null is based on a **normal distribution** (instead of a t-distribution). 

The test statistic, Z, is the difference in means of the outcome between the two groups, divided by the **standard error**. The standard error then is a function of the sample standard deviation of the outcome in each group and the sample size in each group. I've calculated the appropriate standard error in the following code:

```{r}

# The first thing we need is the "pooled" standard deviation from the two
# samples. We know this will be based on the arm specific sample sizes and
# standard deviations for the outcome, so let's save that information as
# different objects.

  sd1 <- sd    (data_2$eos_glvef[data_2$arm == "Arm A (P)"], na.rm = TRUE)
  n1  <- length(data_2$eos_glvef[data_2$arm == "Arm A (P)"])
  
  sd2 <- sd    (data_2$eos_glvef[data_2$arm == "Arm C (HD)"], na.rm = TRUE)
  n2  <- length(data_2$eos_glvef[data_2$arm == "Arm C (HD)"])
  
  se <- sqrt( # take the square root of...
    (sd1^2 / n1) + (sd2^2 / n2)
    #... the sums of the variances (the SDs squared) in each arm divided by 
    # their respective sample sizes
  )

```

And now we can calculate Z:

```{r}

  mean1 <- mean(data_2$eos_glvef[data_2$arm == "Arm A (P)"],  na.rm = TRUE)
  mean2 <- mean(data_2$eos_glvef[data_2$arm == "Arm C (HD)"], na.rm = TRUE)


  z <- (mean2 - mean1) / se

```

So our value for the Z statistics in our study is `r round(z, 2)`. What do we do with this information? Just like the t-test, we just compare it to the sampling distribution under the null hypothesis of "no difference", which is normally distributed with a mean of zero and a standard deviation of 1. That distribution looks like this:

```{r}

# Get the expected sampling distribution under a null hypothesis of no difference
  
  g1 <- ggplot(data_frame(x = c(-4 , 4)), aes(x = x)) + 
    stat_function(fun = dnorm, args = list(0, 1), 
                  geom = "area", fill = viridis(1), alpha = 0.3) +
    xlim(c(-4 , 4)) +
    xlab("Z") +
    ylab("Density") +
    theme_minimal() 

  g1

```

Next, we can can highlight the middle 95% of this distribution. Values for Z falling outside of this area are the ones we would call "significant" for a 2-sided test. 

```{r}

    g2 <- g1 +
    stat_function(
      xlim = c(1.96 , 4), 
      fun = dnorm, 
      args = list(0, 1), 
      geom = "area", fill = viridis(1)
      ) +
    stat_function(
      xlim = c(-4, -1.96), 
      fun = dnorm, 
      args = list(0, 1), 
      geom = "area", fill = viridis(1)
      )
    
  g2

  
  
```



```{r}

  g3 <- g2 +
    geom_vline(xintercept = 1.96, color = "black", linetype = "dashed") +
    stat_function(
      xlim = c(-4 , 8), 
      fun = dnorm, 
      args = list(1.96, 1), 
      geom = "area", 
      fill = viridis(1, begin = 0.8), 
      alpha = 0.5
      ) +
    xlim(-4, 6)

  g3
  
    g3 <- g2 +
    geom_vline(xintercept = 1.96, color = "black", linetype = "dashed") +
    geom_vline(xintercept = 2.8, color = "black", linetype = "dashed") +
    stat_function(
      xlim = c(-4 , 8), 
      fun = dnorm, 
      args = list(2.8, 1), 
      geom = "area", 
      fill = viridis(1, begin = 0.8), 
      alpha = 0.5
      ) +
    xlim(-4, 6)

  g3 # pnorm(1.96, 2.8, 1) # -qnorm(0.20, 0, 1, lower.tail = T) + 1.96 # = Z
  
      g3 <- g2 +
    geom_vline(xintercept = 1.96, color = "black", linetype = "dashed") +
    geom_vline(xintercept = 3.6, color = "black", linetype = "dashed") +
    stat_function(
      xlim = c(-4 , 8), 
      fun = dnorm, 
      args = list(3.6, 1), 
      geom = "area", 
      fill = viridis(1, begin = 0.8), 
      alpha = 0.5
      ) +
    xlim(-4, 6)

  g3 # -qnorm(0.05, 0, 1, lower.tail = T) + 1.96 # = Z # # pnorm(1.96, 3.6, 1) 
  
  
  
```


And finally we can add our value for Z and compare it to this sampling distribution. 

```{r}

  g3 <- g2 +
    geom_vline(xintercept = z, color = viridis(1, begin = 0.5), size = 2)

  g3

```

So we can see that If the Z were generated under the null model (i.e. a normal distribution with a mean of 0 and a SD of 1), then the value of Z we observed would be out into the right hand tail of that distribution, but not among the "most extreme" 5% of values (if we are counting "extreme" in both directions, i.e. using a 2 sided test). Finally, the actual p-value for the value of Z we observed in our study is:

```{r}

  p_ztest <- pnorm(z, mean = 0, sd = 1, lower.tail = FALSE) * 2
  # This code says: given a normal distribution with a mean of zero and a SD of
  # one, what is the probability of seeing a value for Z as extreme or more
  # extreme as the one we observed? We then multiply this by 2 to reflect the 2
  # sided test.

  round(p_ztest, 2)

```

One thing you might notice is that the p-values for our Z test (`r p_ztest`) and t test (`r round(our_t_test$p.value, 2)`) are different, with the Z test suggesting a "more significant" result. The reason for this is that the Z-test assumes that the pooled SD of the outcome in our sample is the same as the underlying population SD. However, the t-test doesn't make this **assumption**, which is why that result reflects a bit more uncertainty (i.e. results in a slightly higher p-value), which is reflected in the "fatter tails" of the t distribution compared to the normal distribution that the Z test is based on. However, as the sample size increases, the t and Normal distributions become more and more alike, and so the Z test becomes an increasingly good approximation of the t-test. It's important to note that this kind of Z test, one that's based on a normal distribution, can also be used to approximate other important test statistics, such as differences in proportions (for binary outcomes), which is why we've introduced it here.  

# Linear models

Now we will use a linear regression model to estimate our treatment effect in this study. First we'll do the test, and then we'll explain what everything means. Running a linear regression model in R is very simple. The function is called `lm` and it needs 2 arguments. The first argument is a formula that specifies the outcome (which some will refer to as the **dependent variable**), and one or more predictors (sometimes called **independent variables**), which in this case is just the study arm. The other argument is just the dataset we are using. 

So here is what it looks like:

```{r}

  form1 <- as.formula("eos_glvef ~ arm")

  m1 <- lm(formula = form1, data = data_2)
  
# We can also just do all of this in one step, and without naming the arguments,
# which is what people usually do.
  
  m1 <- lm(eos_glvef ~ arm, data_2)
  
  summary(m1)

```

That summary is a bit messy, so I like to use a function called `tab_model` from the `sjPlot` package to produce nicer regression tables. 

```{r}

  tab_model(m1)
 
```

The main thing of interest here is the Estimate (2nd column) for arm (2nd row under predictors). We see that the value for this is 4.31. This reflects the difference in mean GLVEF at the end of the study in the active HD arm compared to the placebo arm. This matches exactly what we observed before. More generally, these **regression coefficients** that we have estimated are interpreted as follows: they are the increase in the mean outcome associated with a one unit increase in the predictor. In this case, we have the predictor, **study arm**, which can take 2 values, HD or placebo. The computer however converts these into numbers, so that the placebo arm observations are coded as a 0 for this variable, and the active arm observations are coded as 1. Thus a "one unit increase" in study arm predictor is equivalent to saying "comparing the active arm to the placebo arm". We will see another example below where a predictor is continuously measured, rather than binary. 

Another way to think of this linear model is that it gives us the straight line through the data points that gives the closet fit to them (we'll come back to what this means in a moment). You might remember that the equation for a straight line is Y = Intercept + Slope*X. In this example, the Slope is the regression coefficient for treatment arm, which we just discussed. The intercept value is, perhaps not surprisingly, the regression coefficient labeled "(Intercept)" in table just above. Remember, the intercept of a line is where it crosses the Y axis, which is where X is equal to zero. We just pointed out that the regression is treating the variable `arm` as zero for the placebo group, and thus the intercept value from our regression is the mean GLVEF for patients in the placebo arm (45.91), which matches what we observed earlier. If I wanted to know what the mean GLVEF value was for those in the active arm, then, following the equation for a line, I would just take the intercept (45.91) and add the slope (4.31) multiplied by 1 (the difference between 0 and 1), resulting in 50.22. Again, that matches what we observed above. 

If we return to the plot above, just for the two groups in this model, we can add a fit line that starts at the mean value in the placebo group (45.91) and has a slope of 4.31.

```{r}

  ggplot(data_2, aes(x = arm, y = eos_glvef)) +
    geom_jitter(width = 0.01, alpha = 0.9) +
    stat_summary(fun.y = mean, geom = "point", shape = 20, size = 10,
                 color = "red", fill = "red") +
    geom_smooth(aes(as.numeric(factor(arm))), method = "lm", se = FALSE,
                color = "black", size = 2) +
    xlab("") +
    ylab("End of Study GLVEF (%)")

```



# Linear model for 3 groups

In the example above, we left out the low dose arm, and focused on the difference in means comparing two groups. However, there will be plenty of situations where you want to test for differences among more than 2 groups. The "classic" test for this is called ANOVA (analysis of variance). The basic idea behind ANOVA is that we can take the total variance for an outcome and decompose it into the within-group and between-group components. The ratio of these two quantities is called F. If F is zero, then none of variance is "explained" by group membership, i.e. there is no difference between the groups. 

```{r}

  overall_mean <- mean(data$eos_glvef, na.rm = TRUE)
  # Group specific means
  m_p  <- mean(data$eos_glvef[data$arm == "Arm A (P)" ],  na.rm = TRUE)
  m_ld <- mean(data$eos_glvef[data$arm == "Arm B (LD)"], na.rm = TRUE)
  m_hd <- mean(data$eos_glvef[data$arm == "Arm C (HD)"], na.rm = TRUE)
  # Group sample sizes
  n_p  <- length(
    data$eos_glvef[data$arm == "Arm A (P)" & !is.na(data$eos_glvef)]
    )
  n_ld <- length(
    data$eos_glvef[data$arm == "Arm B (LD)" & !is.na(data$eos_glvef)]
    )
  n_hd <- length(
    data$eos_glvef[data$arm == "Arm C (HD)" & !is.na(data$eos_glvef)]
    )
  
  k <- 3 # 3 groups
  
  n <- length(data$eos_glvef[!is.na(data$eos_glvef)]) # Total sample size

  # This is the between group variability
  between <- sum(
    n_p  * (m_p  - overall_mean)^2, 
    n_ld * (m_ld - overall_mean)^2, 
    n_hd * (m_hd - overall_mean)^2
    ) / (k - 1)

  # This is the within group variability
  within <- sum(
    (data$eos_glvef[data$arm == "Arm A (P)"  & !is.na(data$eos_glvef)] - m_p)^2, 
    (data$eos_glvef[data$arm == "Arm B (LD)" & !is.na(data$eos_glvef)] - m_ld)^2,
    (data$eos_glvef[data$arm == "Arm C (HD)" & !is.na(data$eos_glvef)] - m_hd)^2
  ) / (n - k)
  
  # This is the F statistic
  f_stat <- between / within
  
  # Degree of freedome, which are the parameters that determines the sampling
  # distribution under the null
  df1 <- k - 1 # Number of groups - 1
  df2 <- n - k # Total sample minus the number of groups

  f_pvalue <- pf(f_stat, df1, df2, lower.tail = FALSE)
  
  
```

So the F statistics is `r f_stat`, which has a p-value of `r f_pvalue`. Let's look at this against the backdrop of the sampling distribution under the null:

```{r}

# Get the expected sampling distribution under a null hypothesis of no difference
  
  g1 <- ggplot(data_frame(x = c(0, 4)), aes(x = x)) + 
    stat_function(fun = df, args = list(df1, df2), 
                  geom = "area", fill = viridis(1), alpha = 0.3) +
    xlim(c(0 , 4)) +
    xlab("F") +
    ylab("Density") +
    theme_minimal() +
    geom_vline(xintercept = f_stat, color = viridis(1, begin = 0.5), size = 2)

  g1

```

The good news is that, like with most things in R, there is a function that does all of this for us, which is helpfully named `anova`. 

```{r}

  anova(lm(eos_glvef ~ arm, data))

```
Looking above, you can see the "Mean sq" values for `between` ("arm") and `within` ("Residuals") groups that we calculated above, as well as the F statistic and p-value we already arrived at. 

You might have noticed that when we called the function `anova`, the only argument was another call to the linear model function, `lm`. That's because running an anova and a linear model is basically the same thing. So let's look more closely at that linear model, which is just like the one we ran previously, but now our predictor has 3 values, not 2.

```{r}

  summary(lm(eos_glvef ~ arm, data))

```

If you look at the bottom of that output, we see the same F test and p-value that we've already seen twice before. However, we also get an estimate for the treatment effect both both of the active arms (low does and high dose), relative to the placebo arm. 

# A linear model with a continuous predictor

Now let's go back to looking at the placebo vs the high dose arms only. We are going to add one more predictor to this model, which is the GLVEF measured at baseline (`bl_glvef`). 

```{r}
 
  m2 <- lm(eos_glvef ~ arm + scale(bl_glvef, scale = FALSE), data_2)
  # Note that I have rescaled the baseline GLVEF variable by "mean-centering" 
  # it. This is the improve the interpretation of the intercept in the model 
  # below. Remember that the intercept is the value of the mean of the outcome
  # when all the predictors are equal to zero. However, setting the predictor
  # baseline GLVEF to zero is nonsensical. So by subtracting the mean (which
  # is what mean-centering does), now a zero value for baseline GLVEF is equal
  # to zero, meaning that the intercept in the model below would be the 
  # predicted outcome value for a person in the placebo arm AND with the mean
  # value for baseline GLVEF. 

  summary(m2)
  
```

One of the things you might notice is that the estimate for the treatment effect is not practically zero, whereas before it was about 4 percentage points. The reason for this is that the high dose arm patients, just by "luck of the draw", happened to have higher GLVEF values at baseline, before any treatment was given. You can also see from the regression result that there is a strong positive association between GLVEF measured at baseline with GEVEF measured at the end of the study, which of course makes sense. More specifically, for every one unit increase in baseline GLVEF, we would predict that GLVEF at the end of the study would be 0.7 units higher. So if the patients in the HD arm happened to have higher values at baseline, we should also expect them to have higher values at the end of the study, even if there was no effect of the treatment. In other words, those patients have had a sort of a head start with respect to the outcome. However, by including baseline GLVEF in the model, it "adjusts" for it, effectively removing it's influence on the outcome, and thus erasing the head start.  

Finally, we can look at the model by plotting baseline and end of study against each other, and then fitting the regression lines for each arm. 

```{r}

  data_2$predm2[!is.na(data_2$eos_glvef) & !is.na(data_2$bl_glvef)] <- predict(m2)

  ggplot(data_2, aes(x = bl_glvef, y = eos_glvef, color = arm)) +
    geom_point() +
    geom_line(aes(y = predm2)) +
    scale_color_brewer("", palette = "Set1") +
    xlab("Baseline GLVEF (%)") +
    ylab("End of Study GLVEF (%)")
```

If you only see one line, that's because there was basically no difference between the two arms, which we learned from the model above, so their lines are sitting on top of each other. If there was a notable difference, the vertical distance between the lines correspond with the estimated treatment effect. We can also see that the slope of those lines is positive, but less that one. Of course we know from the model results that it equal to about 0.7 (the coefficient for baseline GLVEF). 

On a final point, you might be interested in how the model "decides" where the line goes. In a nutshell, it puts the line in a place where the sum of squared differences from the line (i.e. all the vertical differences between each observed points and the line) is minimized. You can see a [nice visualization of this here](https://joshualoftus.com/posts/2020-11-23-least-squares-as-springs/). 
