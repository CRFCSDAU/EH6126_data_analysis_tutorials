---
title: "Change Scores"
output: 
  html_document:
    keep_md: true
---

```{r setup, include = FALSE}

  knitr::opts_chunk$set(message = FALSE, warning = FALSE)

# Install/load packages

  packs <- c("tidyverse", "knitr", "viridis", "faux", "sjPlot")
  install.packages(packs[!packs %in% installed.packages()])
  lapply(packs, library, character.only = TRUE)
  
```

Clinical trials are used to estimate the average effect of some treatment. This estimate is derived by comparing one or more groups with respect to some important *outcome*, where the groups are thought to be *comparable* because they were randomized. 

However, while we know that randomization will result in groups that are comparable on average, over many hypothetical trials, we don't actually run many hypothetical trials - we just have the one trial that we actually ran. And for that one trial we might actually have a situation where there really are important differences between the groups (i.e. they aren't that comparable after all) that could lead to errors of inferences (e.g. concluding the treatment is beneficial when it isn't).

When we talk about *comparability* like this, what we really mean is that in the *absence* of any intervention, the distribution of the outcome measured at the end of the study will be the same in all the groups. 

A key challenge is that we can never truly *know* how comparable our groups are - we have to assume it. This is because we can't observe the distribution of future outcomes in the absence of any intervention for every group, because we will always have at least one group where we intervened (or else what's the point in running the trial?).

However, we can use what we know about the outcome, and in particular what factors predict it, and then use measurements of those factors measured at baseline to predict future outcomes. Of all of these predictors, one of the strongest is called *the* baseline, which is a measure of the outcome measured at the start of the study, usually just before randomization. 

For example, say we have a trial for a blood pressure medication. The outcome then is continuously measured blood pressure, and we are hoping that the medication will lower patients' BP values. So we set up the trial, recruit some patients and randomize them into two groups. Then we give one group the new medication we are testing, and the other gets standard of care. At the end of the study we compare the mean blood pressure of the two groups and find that the active group had a blood pressure that was 3 mmHg lower, on average, than the values seen in the control group. We might thus conclude that the treatment worked. 

However, what if it just so happened that the active group also had a similarly lower mean blood pressure (vs the other group) before the study even started? Again, randomization means that on average, across many hypothetical randomization this won't be the case - the groups should usually have very similar distributions of blood pressures at baseline. But you can always get "unlucky" with any one particular randomization, and wind up with a baseline difference that could throw off your conclusions. How to best deal with this situation is one of the most common topics for discussion and debate among clinical trialists. 

One very commonly employed tactic for dealing with this problem is to analyze change scores. This means that you subtract the baseline value of the outcome from the value measured at the end of the study and use that difference for your statistical tests or models. THe apparent logic is that by taking the difference between the baseline and end of study outcome measurements, you are accounting for any important differences between the groups at baseline. Let's see if that actually works in practice. 

First, let's simulate some data when we know there is no treatment effect. 

```{r}

# Set the seed for the random number generator to preserve reproducability
  set.seed(1209) 

# Now generate baseline and end-of-study SBP for 10000 people where the mean is
# 124.5 mmHg, and successive measurements have a correlation = 0.5.

  sbp <- rnorm_multi(
    n = 10000,          # 10k observations
    mu = rep(124.5, 2), # Same mean
    sd = rep(18, 2),    # Same SD
    r = c(0.5),         # Correlation = 0.5
    varnames = c("sbp_eos", "sbp_bl"),
    empirical = FALSE
    )
  
```

```{r}
# Now check the data to see if it is what we wanted. 
  
  map(sbp, mean) # Are both means close to 124.5?

```
```{r}

  map(sbp, sd) # Are both SDs close to 18?

```

```{r}

  with(sbp, cor(sbp_eos, sbp_bl)) # Is the correlation between the 2 vars ~ 0.5?

```
So far, so good. How let's randomly assign everyone to one of two study arms. 

```{r}

  sbp$arm <- sample(c("Active", "Control"), size = 10000, replace = TRUE) %>%
    factor(levels = c("Control", "Active"))

```

```{r}

  table(sbp$arm) # Check how many went into each arm

```

Finally, let's see if there is a difference in either blood pressure values between arms. To do this, we will use a simple linear regression model. 

```{r}

  m1_eos <- lm(sbp_eos ~ arm, data = sbp)
  m1_bl  <- lm(sbp_bl  ~ arm, data = sbp)
  
  tab_model(m1_eos, m1_bl)

```

The models both confirm what we already knew - that there is essentially no difference in mean blood pressure, measured at baseline or the end of study, between the two groups. 

Given the large number of observations we just simulated, it shouldn't come as a surprise that we didn't see differences between the two groups. After all, as the sample size increases, the probability of any particular randomization having a sizable "imbalance" plummets. To better understand this, let's repeat the simulation but now with a much smaller sample size. 

```{r}

# Data
  sbp <- rnorm_multi(                           # Generate the SBP measures
    n = 20, # Smaller sample size
    mu = rep(124.5, 2),
    sd = rep(18, 2),
    r = c(0.5),
    varnames = c("sbp_eos", "sbp_bl"),
    empirical = FALSE
    ) %>%
  mutate(arm =                                  # Randomize to study arm
    factor(
      sample(c("Active", "Control"), size = 20, replace = TRUE), 
      levels = c("Control", "Active")
      )
    )

# Models
  m1_eos <- lm(sbp_eos ~ arm, data = sbp)
  m1_bl  <- lm(sbp_bl  ~ arm, data = sbp)
  
  tab_model(m1_eos, m1_bl)


```

Look! The p-values are still large, but the estimates (i.e. the difference in mean values between arms) are now much much larger. Let's take a closer look at imbalances in the baseline measures as a function of sample size. We'll do this by simulating many datasets with different sample sizes, each time calculating the mean difference in baseline SBP between the groups, and then plotting those differences.   

```{r}

# Make a function to simulate the data, and then the difference in means

  gen_diffs <- function(n){
    
    data <- data_frame(                           
      sbp_bl = rnorm(n, 124.5, 18),  
      arm = factor(
        sample(c("Active", "Control"), size = n, replace = TRUE), 
        levels = c("Control", "Active")
        )
      )
    
    diff <- mean(data$sbp_bl[data$arm == "Active"]) -  
      mean(data$sbp_bl[data$arm == "Control"])
    
    return(diff)
  }

```

```{r}

# Use that function repeatedly to make all the different simulated samples

  sizes = c(10, 20, 50, 100, 200, 500, 1000)
  names(sizes) <- sizes

  diffs <- map_df(
    sizes, 
    function(x) replicate(1000, gen_diffs(x))
    )

```


```{r}

  diffs %>%
    gather(n, Difference) %>%
    mutate(n = factor(n, levels = as.character(sizes))) %>%
  ggplot(aes(y = Difference, x = n, color = n)) +
    geom_boxplot(outlier.alpha = 0) +
    geom_jitter(alpha = 0.1) +
    scale_color_viridis(guide = FALSE, discrete = TRUE) +
    ylab("Between arm difference in mean SBP")
    

```


